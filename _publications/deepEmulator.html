---
layout: archive
title: "Publication"
permalink: /publications/deepEmulator.html
author_profile: true
---

<style type="text/css">
    @import url("./web.css");
</style>

<div class="title_section">
    <div style="text-align:center;">
        <h1 class="papertitle">A Deep Emulator for <br>Secondary Motion of 3D Characters</h1>
	<div class="authors">CVPR, 2021. (Oral Presentation)</div>
        <div class="authors">Mianlun Zheng<sup>1</sup>, Yi Zhou<sup>2</sup>, Duygu Ceylan<sup>2</sup>, Jernej Barbi&#269<sup>1</sup></div>
        <div class="authors"><i> <sup>1</sup> University of Southern California, <sup>2</sup>Adobe Research</i> </div>
        <div class="paper_link">
            <a href="https://arxiv.org/abs/2103.01261" style="color:white"> [paper (arXiv)] </a>
	    <a href="https://github.com/ZhengMianlun/deep_emulator" style="color:white"> [code] </a>
        </div>
    </div>
</div>

<p class="abstract">
<b>Abstract</b>: Fast and light-weight methods for animating 3D characters are desirable in various applications such as computer games. We present a learning-based approach to enhance skinning-based animations of 3D characters with vivid secondary motion effects. We design a neural network that encodes each local patch of a character simulation mesh where the edges implicitly encode the internal forces between the neighboring vertices. The network emulates the ordinary differential equations of the character dynamics, predicting new vertex positions from the current accelerations, velocities and positions. Being a local method, our network is independent of the mesh topology and generalizes to arbitrarily shaped 3D character meshes at test time. We further represent per-vertex constraints and material properties such as stiffness, enabling us to easily adjust the dynamics in different parts of the mesh. We evaluate our method on various character meshes and complex motion sequences. Our method can be over 30 times more efficient than ground-truth physically based simulation, and outperforms alternative solutions that provide fast approximations.
</p>

<div class="list_section">
    <ul role="list" style="margin-top: 15px; margin-bottom: 0px">
        <li class="list-item" style="margin-top: 0px; margin-bottom: 0px">
            <a href="#experiment_1">1. Training data examples</a>
        </li>
        <li class="list-item" style="margin-top: -5px; margin-bottom: 0px">
            <a href="#experiment_2">2. Our Secondary Motion inference pipeline</a>
        </li>

        <li class="list-item" style="margin-top: -5px; margin-bottom: 0px">
            <a href="#experiment_3" style="margin-top: -5px; margin-bottom: 0px">3. Our results</a>
        </li>
        <li class="list-item" style="margin-top: -5px; margin-bottom: 0px">
            <ul role="list" style="margin-top: -5px; margin-bottom: 0px">
                <li style="margin-top: -5px; margin-bottom: 0px">
                    <a href="#experiment_3_1">3.1 Homogeneous dynamics</a>
                </li>
                <li style="margin-top: -5px; margin-bottom: 0px">
                    <a href="#experiment_3_2">3.2 Non-homogeneous dynamics</a>
                </li>
		<li style="margin-top: -5px; margin-bottom: 0px">
                    <a href="#experiment_3_3">3.3 Performance analysis</a>
                </li>
            </ul>
        </li>
        <li class="list-item" style="margin-top: -5px; margin-bottom: 0px">
            <a href="#experiment_4">4. Ablation study and comparisons</a>
        </li>
        <li class="list-item" style="margin-top: -5px; margin-bottom: 0px">
            <a href="#experiment_5">5. Limitations</a>
        </li>
        <li class="list-item" style="margin-top: -5px; margin-bottom: 0px">
            <a href="#experiment_6">6. Easter Egg</a>
        </li>
    </ul>
</div>


<div class="w-container">
    <h2 id="experiment_1">1. Training data examples</h2>
    <p class="text"> We construct our training data by assigning randomized motions to a primitive, e.g. a sphere.
        All results in the later sections are predicted from the network trained on this dataset. We here provide three
        random motion examples of the sphere dataset. The color coding shows the dynamics.
    </p>
    <p class="text_italic"> Sphere (vtx#: 1015; motion: random; frames: 456): </p>
    <video width="100%" height="100%" controls autoplay source="" src="https://drive.google.com/uc?id=13o3wmwmWRHe3CvZGuod4eG0YyKokMCiE" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
    <!-- <iframe src="https://drive.google.com/file/d/13o3wmwmWRHe3CvZGuod4eG0YyKokMCiE/preview" width="640" height="480" ></iframe> -->
</div>


<div>
    <div class="w-container">
        <h2 id="experiment_2">2. Our Secondary Motion inference pipeline</h2>
        <p class="text"> Below is the basic pipeline of our method. Given the skinned animation mesh, we first
            build a uniform volumetric mesh and then set the constraints. At inference time, our network predics the
            secondary motion with respect to the input skinned volumetric mesh. Finally, we render the surface mesh
            interpolated from the predicted volumetric mesh.</p>
        <div style="text-align:center"><img src="https://drive.google.com/uc?id=1vfYceyiN-PIWuOPFU89stmcTCzch3lke" width="100%" height="100%"></div>
        <!-- <iframe src="https://drive.google.com/file/d/1vfYceyiN-PIWuOPFU89stmcTCzch3lke/preview" width="640" height="480" style="border: none"></iframe> -->
    </div>
</div>

<div>
    <div class="w-container">
        <h2 id="experiment_3">3. Our results</h2>
    </div>
    <div class="w-container">
        <p class="text_italic">Big vegas (Figure 1) (vtx#: 1468): <br> Running time: 0.012 s/frame on GPU; 0.017 s/frame on CPU.</p>
        <video width="100%" height="100%" source="" src="https://drive.google.com/uc?id=1um5xgGY3SCsYlRL_7XqetEHdNl3wdRhW" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
        <!-- <iframe src="https://drive.google.com/file/d/1um5xgGY3SCsYlRL_7XqetEHdNl3wdRhW/preview" width="640" height="480"></iframe> -->
    </div>
    
    <div class="w-container">
        <p class="text_italic">Big vegas (vtx#: 39684): <br> Running time: 0.14 s/frame on GPU; 0.89 s/frame on CPU.</p>
        <video width="100%" height="100%" source="" src="https://drive.google.com/uc?id=1vcRmuHyNpD-sAfz3Mv0LTbeE-65ymfJm" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
        <!-- <iframe src="https://drive.google.com/file/d/1um5xgGY3SCsYlRL_7XqetEHdNl3wdRhW/preview" width="640" height="480"></iframe> -->
    </div>

    <div class="w-container">
        <h2 id="experiment_3_1">3.1 Homogeneous dynamics</h2>	
	<p class="text_italic">Ortiz (vtx#: 1258)</p>
	<div class="inline" style="margin-top:-20px">
	  <ul style="display: table-row">
	    <li style="display: table-cell"><p class="text_italic">Motion: cross jumps rotation; frames: 122:</p>
		<div style="text-align:center">
		<video width="97%" source="" src="https://drive.google.com/uc?id=1R22eiee1cSXLJUO4L_Y0hnaj3gRFb4yH" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
		</div>
	    </li>

	    <li style="display: table-cell"><p class="text_italic">Motion: jazz dancing; frames: 326:</p>
		<div style="text-align:center">	      
		<video width="97%" source="" src="https://drive.google.com/uc?id=19_8Uz7ZLJaK5Y_XOBROpWN6shZfBxecb" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
		</div>
	    </li>
	  </ul>
	</div>

        <p class="text_italic">Kaya (vtx#: 1417) </p>
	<div class="inline" style="margin-top:-20px">
	  <ul style="display: table-row">
	    <li style="display: table-cell"><p class="text_italic">Motion: zombie scream; frames: 167:</p>
	      <div style="text-align:center">	  
	      <video width="97%" source="" src="https://drive.google.com/uc?id=12YJ8E0AhXHMUMDIb0BN5sR5UkWNJwHrH" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
		</div>
	    </li>

	    <li style="display: table-cell"><p class="text_italic">Motion: dancing running man; frames: 240:</p>	
	      <div style="text-align:center">	  
	      <video width="97%" source="" src="https://drive.google.com/uc?id=13opn7L0cDg3agI1bw9ltRD4DviDQyFMn" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
		</div>
	    </li>
	  </ul>
	</div>
    </div>

    <div class="w-container">
        <h2 id="experiment_3_2">3.2 Non-homogeneous dynamics</h2>
        <p class="text"> As described in Figure 6, we set three different material properties to the character. In the legend at the bottom of the video, the gray area is the constraint, the red area has lower stiffness and the pink area has higher stiffness.</p>

        <p class="text_italic">Michelle (Figure 6) (vtx#: 1105; motion: cross jumps; frames: 122):</p>
        <video width="100%" height="100%" source="" src="https://drive.google.com/uc?id=1GOgMifGz_eyxDZ0k5oTVzjU4Ly6MCnHH" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
        <!-- <iframe src="https://drive.google.com/file/d/1GOgMifGz_eyxDZ0k5oTVzjU4Ly6MCnHH/preview" width="640" height="480"></iframe> -->

        <p class="text_italic">Michelle (vtx#: 1105; motion: gangnam style; frames: 371):</p>
        <video width="100%" height="100%" source="" src="https://drive.google.com/uc?id=1dqP2kH8txB3o4ERz473J4T2l9Ce-PJUy" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
        <!-- <iframe src="https://drive.google.com/file/d/1dqP2kH8txB3o4ERz473J4T2l9Ce-PJUy/preview" width="640" height="480"></iframe> -->

        <p class="text_italic">Big vegas (vtx#: 1468; motion; cross jumps rotation; frames: 122): </p>
        <video width="100%" height="100%" source="" src="https://drive.google.com/uc?id=1vnr-HI1FC-eUAVsuXkyNBPzBSHEFU2dg" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
        <!-- <iframe src="https://drive.google.com/file/d/1vnr-HI1FC-eUAVsuXkyNBPzBSHEFU2dg/preview" width="640" height="480"></iframe> -->

    </div>

    <div class="w-container">
	<h2 id="experiment_3_3">3.3 Performance analysis</h2>
	<p class="text"> In Table 1, we show the speed t_ours of our method, as well as that of the ground truth method t_GT and a baseline method t_BL. We adopted the implicit backward Euler approach as ground truth and the faster explicit central differences integration as the baseline. For each method, we record the time to calculate the dynamic mesh but exclude other components such as initialization, rendering and mesh interpolation. Results indicate that our method, ran on GPU (CPU) is around 30(~20) times faster than the implicit integrator and 3(~2) times faster than the explicit integrator, per frame. With vertices number increasing, the performance of our method is rather more competitive.</p>
	<div style="text-align:center">
	<img src="https://drive.google.com/uc?id=1gNgYaIPBNSiZEe9Z2i1SNXtTU25Jx3le" width="100%" height="100%">
	</div>
</div>

<div>
    <div class="w-container">
        <h2 id="experiment_4">4. Ablation study and comparions</h2>
    </div>
    <div class="w-container">
        <p class="text_italic">Big vegas (Figure 7) (vtx#: 1468; motion: hip hop dancing; frames: 283): </p>
        <video width="100%" height="100%" source="" src="https://drive.google.com/uc?id=1cH3jfL33yzLhqc_vWMNc2GHUL3Ku6EKa" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
        <!-- <iframe src="https://drive.google.com/file/d/1cH3jfL33yzLhqc_vWMNc2GHUL3Ku6EKa/preview" width="640" height="480"></iframe> -->
    </div>
</div>

<div>
    <div class="w-container">
        <h2 id="experiment_5">5. Limitations</h2>
        <p class="text"> As discussed in the Conclusion Section, if the local geometric detail of a character is
            significantly different to those seen during training, e.g., the main local structures in the ear regions of
            the mousey character don't appear in the volumetric
            mesh of the sphere for training, the quality of our output decreases. One potential avenue for addressing
            this is to design more general primitives for training, beyond the tetrahedralized sphere. A thorough study
            on the type of training
            primitives and motion sequences can be an interesting future direction.</p>
        <p class="text_italic">Mousey (vtx#: 2303; motion: swing dancing; frames: 627):</p>
        <video width="100%" height="100%" source="" src="https://drive.google.com/uc?id=1v7VH9MN42MCerwN2walv2QlpX01Hr4bl" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
        <!-- <iframe src="https://drive.google.com/file/d/1v7VH9MN42MCerwN2walv2QlpX01Hr4bl/preview" width="640" height="480"></iframe> -->

    </div>
</div>

<div>
    <div class="w-container">
        <h2 id="experiment_6">6. Easter egg</h2>
        <p class="text_italic">15 different animations of five different characters with predicted secondary motion:</p>
        <video width="100%" height="100%" source="" src="https://drive.google.com/uc?id=1gTDKpNJx7i99xsCI-zRLDT5wECKldmZp" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
        <!-- <iframe src="https://drive.google.com/file/d/1gTDKpNJx7i99xsCI-zRLDT5wECKldmZp/preview" width="640" height="480"></iframe> -->

        <p class="text"> <br>Thanks! Stay healthy and happy.</p>
    </div>
</div>
